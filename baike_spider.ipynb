{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request        #导入urllib.request库\n",
    "import urllib.parse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def delete_tags(htmlString):\n",
    "    s2 = re.sub(r'<.*?>','',htmlString)\n",
    "    s2 = s2.replace('\\n','')\n",
    "    return s2\n",
    "\n",
    "catalog=[]\n",
    "path2=input('请输入目录文件路径：')\n",
    "with open(path2,'r',encoding='GB2312') as f:\n",
    "    for line in f:\n",
    "        catalog.append(list(line.strip('\\n').split(',')))\n",
    "for _item in catalog:\n",
    "    print(\"开始爬取词条：\"+ _item[0])\n",
    "    c1=urllib.parse.quote(_item[0])\n",
    "    b = \"https://baike.baidu.com/item/\"+c1\n",
    "    a = urllib.request.urlopen(b)#打开指定网址\n",
    "    html = a.read()              #读取网页源码\n",
    "    html = html.decode(\"utf-8\") #解码为unicode码\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    div = soup.find('div',class_='main-content')\n",
    "    pattern1=\"(?<=\\/span>).*(?=<\\/h2>)\"  #匹配小标题\n",
    "    pattern2=\"(?<=label-module=\\\"para\\\">).*(?=<\\/div>)\"   #匹配内容\n",
    "\n",
    "    #观察发现，小标题(\"适应症\"等)单独写在一个class='para_title level-2'的div里\n",
    "    #它的内容写在若干个和小标题平级的class=\"para\"的div里，这些div是紧密排列的\n",
    "    #它们的上级div可以通过class=\"main-content\"索引到\n",
    "    #对于一个项（\"适应症\"等下面所有内容）直到下一个项，排列格式是：\n",
    "        #一个<div class=\"anchor-list\">…</div>(意义不明)\n",
    "        #一个<div class=\"para-title level-2\">…</div>\n",
    "        #若干个<div class=\"para\">…<div>\n",
    "    all_contents = div.contents\n",
    "    #print(all_contents[1])\n",
    "    my_set=[]\n",
    "    for i in all_contents:\n",
    "        my_set.append(str(i))\n",
    "\n",
    "    temp_content=''\n",
    "    temp_title=''\n",
    "    content_set=[]\n",
    "    title_set=[]\n",
    "\n",
    "    num=[]\n",
    "\n",
    "    for j in my_set:\n",
    "        try: #先判断是不是小标题\n",
    "            print(\"------\"+delete_tags(re.search(pattern1,j).group())+\"------\")\n",
    "            if(temp_content!=''):\n",
    "                content_set.append(temp_content)\n",
    "                temp_content=''\n",
    "                title_set.append(temp_title)\n",
    "            temp_title=delete_tags(re.search(pattern1,j).group())\n",
    "\n",
    "            num.append(\"1\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                print(\">>>\"+delete_tags(re.search(pattern2,j).group()))\n",
    "                temp_content=temp_content+delete_tags(re.search(pattern2,j).group())\n",
    "\n",
    "                num.append(\"2\")\n",
    "\n",
    "            except Exception as ee:\n",
    "                num.append(\"0\")\n",
    "\n",
    "    content_set.append(temp_content)\n",
    "    title_set.append(temp_title)\n",
    "    df1=pd.DataFrame(columns=['内容'])\n",
    "    for i in range(0,len(title_set)):\n",
    "        df1.loc[title_set[i]]=content_set[i]\n",
    "    path1=_item[0]+\".csv\"\n",
    "\n",
    "    try:\n",
    "        df1.to_csv(path1,encoding='utf_8_sig')\n",
    "        print(\"\\n爬取成功！文件存于 \"+_item[0]+\".csv\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n爬取失败\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
